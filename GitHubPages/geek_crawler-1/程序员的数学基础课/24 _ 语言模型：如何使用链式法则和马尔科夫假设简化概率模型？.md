<audio title="24 _ 语言模型：如何使用链式法则和马尔科夫假设简化概率模型？" src="https://static001.geekbang.org/resource/audio/20/fc/207b007240e15cf081d84c73e79f67fc.mp3" controls="controls"></audio> 
<p>你好，我是黄申。</p><p>之前我给你介绍了用于分类的朴素贝叶斯算法。我们讲了，朴素贝叶斯算法可以利用贝叶斯定理和变量之间的独立性，预测一篇文章属于某个分类的概率。除了朴素贝叶斯分类，概率的知识还广泛地运用在其他机器学习算法中，例如语言模型、马尔科夫模型、决策树等等。</p><p>今天我就来说说，基于概率和统计的语言模型。语言模型在不同的领域、不同的学派都有不同的定义和实现，因此为了避免歧义，我这里先说明一下，我们谈到的语言模型，都是指基于概率和统计的模型。</p><h2>语言模型是什么？</h2><p>在解释语言模型之前，我们先来看两个重要的概念。第一个是链式法则，第二个是马尔科夫假设及其对应的多元文法模型。为什么要先说这两个概念呢？这是因为链式法则可以把联合概率转化为条件概率，而马尔科夫假设通过变量间的独立性来减少条件概率中的随机变量，两者结合就可以大幅简化计算的复杂度。</p><h3>1.链式法则</h3><p>链式法则是概率论中一个常用法则。它使用一系列条件概率和边缘概率，来推导联合概率，我用一个公式来给你看看它的具体表现形式。</p><p><img src="https://static001.geekbang.org/resource/image/de/0c/de5d37d9392a18225b4f9d522b4f180c.png?wh=554*42" alt=""></p><p>其中，$x_{1}$到$x_{n}$表示了n个随机变量。</p><!-- [[[read_end]]] --><p>这个公式是怎么来的呢？你还记得联合概率、条件概率和边缘概率之间的“三角”关系吗？我们用这三者的关系来推导一下，最终我们可以得到链式法则。</p><p><img src="https://static001.geekbang.org/resource/image/35/a2/35bbc4bd80d8fba7bd6a63c1cdbd86a2.png?wh=475*149" alt=""></p><p>推导的每一步，都是使用了三种概率之间的关系，这个应该不难理解。</p><h3>2.马尔科夫假设</h3><p>理解了链式法则，我们再来看看马尔可夫假设。这个假设的内容是：任何一个词$w_{i}$出现的概率只和它前面的1个或若干个词有关。基于这个假设，我们可以提出<strong>多元文法（Ngram）模型</strong>。Ngram中的“N”很重要，它表示任何一个词出现的概率，只和它前面的N-1个词有关。</p><p>我以二元文法模型为例，来给你解释。按照刚才的说法，二元文法表示，某个单词出现的概率只和它前面的1个单词有关。也就是说，即使某个单词出现在一个很长的句子中，我们也只需要看前面那1个单词。用公式来表示出来就是这样：</p><p><img src="https://static001.geekbang.org/resource/image/b0/e9/b0bc4bb97ba4b233e0ae9db0291059e9.png?wh=263*39" alt=""></p><p>如果是三元文法，就说明某个单词出现的概率只和它前面的2个单词有关。即使某个单词出现在很长的一个句子中，它也只看相邻的前2个单词。用公式来表达就是这样：</p><p><img src="https://static001.geekbang.org/resource/image/a6/94/a619504d5c698f0c519904cd6eddf094.png?wh=302*37" alt=""></p><p>你也许会好奇，那么一元文法呢？按照字面的意思，就是每个单词出现的概率和前面0个单词有关。这其实说明，每个词的出现都是相互独立的。用公式来表达就是这样的：</p><p><img src="https://static001.geekbang.org/resource/image/52/25/523ff06450418d4d91d8e6d2756e1025.png?wh=400*40" alt=""></p><p>弄明白链式法则和马尔科夫假设之后，我们现在来看语言模型。</p><p>假设我们有一个统计样本文本$d$，$s$表示某个有意义的句子，由一连串按照特定顺序排列的词$w_{1}，w_{2},…,w_{n}$组成，这里$n$是句子里单词的数量。现在，我们想知道根据文档d的统计数据，$s$在文本中出现的可能性，即$P(s|d)$，那么我们可以把它表示为$P(s|d)=P(w_{1}, w_{2}, …, w_{n} | d$)。假设我们这里考虑的都是在集合d的情况下发生的概率，所以可以忽略$d$，写为$P(s)=P(w_{1}, w_{2}, …, w_{n})$。</p><p>到这里，我们碰到了第一个难题，就是如何计算$P(w_{1}, w_{2}, …, w_{n})$要在集合中找到一模一样的句子，基本是不可能的。这个时候，我们就需要使用链式法则。我们可以把这个式子改写为：</p><p><img src="https://static001.geekbang.org/resource/image/2f/23/2fe51ac1be8efbad2082627860aada23.png?wh=733*40" alt=""></p><p>乍一看，问题似乎是解决了。因为通过文档集合C，你可以知道$P(w_{1})$，$P(w_{2}|w_{1})$这种概率。不过，再往后看，好像$P(w_{3}|w_{1},w_{2})$出现概率很低，$P(w_{4}|w_{1},w_{2},w_{3})$出现的概率就更低了。一直到$P(w_{n}|w_{1}, w_{2}, …, w_{n-1})$，基本上又为0了。我们可以使用上一节提到的平滑技巧，减少0概率的出现。不过，如果太多的概率都是通过平滑的方式而得到的，那么模型和真实的数据分布之间的差距就会加大，最终预测的效果也会很差，所以平滑也不是解决0概率的最终办法。</p><p>除此之外，$P(w_{1}, w_{2}, …, w_{n})和P(w_{n}|w_{1}, w_{2}, …, w_{n-1})$还不只会导致0概率，它还会使得模型存储空间的急速增加。</p><p>为了统计现有文档集合中$P(w_{1}, w_{2}, …, w_{n})$这类值，我们就需要生成很多的计数器。我们假设文档集合中有m个不同的单词，那么从中挑出$n$个单词的可重复排列，数量就是$m^{n}$。此外，还有$m^{n-1}$, $m^{n-2}$等等。这也意味着，如果要统计并存储的所有$P(w_{1}, w_{2}, …, w_{n})$或$P(w_{n}|w_{1}, w_{2}, …, w_{n-1})$这类概率，就需要大量的内存和磁盘空间。当然，你可以做一些简化，不考虑单词出现的顺序，那么问题就变成了可重复组合，但是数量仍然非常巨大。</p><p>如何解决0概率和高复杂度的问题呢？马尔科夫假设和多元文法模型能帮上大忙了。如果我们使用三元文法模型，上述公式可以改写为：</p><p><img src="https://static001.geekbang.org/resource/image/6d/b1/6dfe1f3c267787677090a611fee964b1.png?wh=651*38" alt=""></p><p>这样，系统的复杂度大致在(C(m, 1) + C(m, 2) + C(m, 3))这个数量级，而且$P(w_{n}|w_{n-2}, w_{n-1})$为0的概率也会大大低于$P(w_{n}|w_{1}, w_{2}, …, w_{n-1})$ （其中$n&gt;&gt;3$）为0的概率。当然，多元文法模型中的N还是不能太大。随着N的增大，系统复杂度仍然会快速升高，就无法体现出多元文法的优势了。</p><h2>语言模型的应用</h2><p>基于概率的语言模型，本身不是新兴的技术。它已经在机器翻译、语音识别和中文分词中得到了成功应用。近几年来，人们也开始在信息检索领域中尝试语言模型。下面我就来讲讲语言模型在信息检索和中文分词这两个方面是如何发挥作用的。</p><h3>1.信息检索</h3><p>信息检索很关心的一个问题就是相关性，也就是说，给定一个查询，哪篇文档是更相关的呢？为了解决相关性问题，布尔模型和向量空间检索模型都是从查询的角度出发，观察查询和文档之间的相似程度，并以此来决定如何找出相关的文档。这里的“相似程度”，你可以理解为两者长得有多像。那么，语言模型如何来刻画查询和文档之间的相关度呢？</p><p>它不再使用相似度定义，而是采用了概率。一种常见的做法是计算$P(d|q)$，其中$q$表示一个查询，$d$表示一篇文档。$P(d|q)$表示用户输入查询$q$的情况下，文档$d$出现的概率是多少？如果这个概率越高，我们就认为$q$和$d$之间的相关性越高。</p><p>通过我们手头的文档集合，并不能直接获得$P(d|q)$。好在我们已经学习过了贝叶斯定理，通过这个定理，我们可以将$P(d|q)$重写如下：</p><p><img src="https://static001.geekbang.org/resource/image/de/67/de8d794df4e2758063f5ffb26546c467.png?wh=197*66" alt=""></p><p>对于同一个查询，其出现概率$P(q)$都是相同的，同一个文档$d$的出现概率$P(d)$也是固定的。因此它们可以忽略，我们只要关注如何计算$P(q|d)$。而语言模型，为我们解决了如何计算$P(q|d)$的问题，让$k_{1}, k_{2}, …, k_{n}$表示查询q里包含的$n$个关键词。那么根据之前的链式法则公式，可以重写为这样：</p><p><img src="https://static001.geekbang.org/resource/image/d7/66/d78ba0db38a575269eff4f6cb89afa66.png?wh=735*54" alt=""></p><p>为了提升效率，我们也使用马尔科夫假设和多元文法。假设是三元文法，那么我们可以写成这样：</p><p><img src="https://static001.geekbang.org/resource/image/e9/13/e977fb4b682e67a8693f1b76f1febe13.png?wh=712*52" alt=""></p><p>最终，当用户输入一个查询$q$之后，对于每一篇文档$d$，我们都能获得$P(d|q)$的值。根据每篇文档所获得的$P(d|q)$这个值，由高到低对所有的文档进行排序。这就是语言模型在信息检索中的常见用法。</p><h3>2.中文分词</h3><p>和拉丁语系不同，中文存在分词的问题。如果想进行分词，你就可以使用语言模型。我举个例子给你解释一下，你就明白了。</p><p>最普遍的分词方法之一是基于常用词的词典。如果一个尚未分词的句子里发现了存在于字典里的词，我们就认为找到一个新的词，并把它切分出来。这种切分不会出现完全离谱的结果，但是无法解决某些歧义。我下面来举个例子，原句是“兵乓球拍卖完了”。我在读的时候，会有所停顿，你就能理解分词应该如何进行。可是，仅仅从书面来看，至少有以下几种分词方式：</p><pre><code>第一种，兵乓|球|拍卖|完了
第二种，兵乓球|拍卖|完了
第三种，兵乓|球拍|卖完|了
第四种，兵乓|球拍|卖|完了
</code></pre><p>上面分词的例子，从字面来看都是合理的，所以这种歧义无法通过这句话本身来解决。那么这种情况下，语言模型能为我们做什么呢？我们知道，语言模型是基于大量的语料来统计的，所以我们可以使用这个模型来估算，哪种情况更合理。</p><p>假设整个文档集合是D，要分词的句子是$s$，分词结果为$w_{1}$, … $w_{n}$，那么我们可以求$P(s)$的概率为：</p><p><img src="https://static001.geekbang.org/resource/image/c7/34/c74b222e54b6c2a9a6385a988896f334.png?wh=809*47" alt=""></p><p>请注意，在信息检索中，我们关心的是每篇文章产生一个句子（也就是查询）的概率，而这里可以是整个文档集合D产生一个句子的概率。</p><p>根据链式法则和三元文法模型，那么上面的式子可以重写为：</p><p><img src="https://static001.geekbang.org/resource/image/d4/be/d4cfe338d5aed842cd1b366008a551be.png?wh=784*44" alt=""></p><p>也就是说，语言模型可以帮我们估计某种分词结果，在文档集合中出现的概率。但是由于不同的分词方法，会导致$w_{1}$到$w_{n}$的不同，因此就会产生不同的$P(s)$。接下来，我们只要取最大的$P(s)$，并假设这种分词方式是最合理的，就可以在一定程度上解决歧义。我们可以使用这个公式来求解：</p><p><img src="https://static001.geekbang.org/resource/image/1c/dc/1cfeaeb018f38f86c448b57b61525adc.png?wh=152*42" alt=""></p><p>其中，$W_{i}$表示第$i$种分词方法。</p><p>回到“兵乓球拍卖完了”这句话，如果文档集合都是讲述的有关体育用品的销售，而不是拍卖行，那么“兵乓|球拍|卖完|了”这种分词的可能性应该更高。</p><h2>小结</h2><p>这一节，我介绍了基于概率论的语言模型，以及它在信息检索和中文分词领域中的应用。这一节的公式比较多，你刚开始看可能觉得有点犯晕。不用急，我给你梳理了几个要点，你只要掌握这几个要点，依次再进行细节学习，就会事半功倍。</p><p>第一，使用联合概率，条件概率和边缘概率的“三角”关系，进行相互推导。链式法则就是很好的体现。</p><p>第二，使用马尔科夫假设，把受较多随机变量影响的条件概率，简化为受较少随机变量影响的条件概率，甚至是边缘概率。</p><p>第三，使用贝叶斯定理，通过先验概率推导后验概率。在信息检索中，给定查询的情况下推导文档的概率，就需要用到这个定理。</p><p>如果你记住了这几点，那么不仅能很快地理解本篇的内容，还能根据实际需求，设计出满足自己需要的语言模型。</p><h2>思考题</h2><p>在中文分词的时候，我们也可以考虑文章的分类。比如，这样一句话“使用纯净水源浇灌的大米”，正确的切分应该是：</p><pre><code>使用|纯净|水源|浇灌|的|大米
</code></pre><p>如果我们知道这句描述来自于“大米”类商品，而不是“纯净水”类商品，那么就不会错误地切分为：</p><pre><code>使用|纯净水|源|浇灌|的|大米
</code></pre><p>想想看，如何对我介绍的语言模型加以改进，把分类信息也包含进去？</p><p><span class="orange">欢迎留言和我分享，也欢迎你在留言区写下今天的学习笔记。你可以点击“请朋友读”，把今天的内容分享给你的好友，和他一起精进。</span></p>